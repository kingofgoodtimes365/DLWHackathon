# -*- coding: utf-8 -*-
"""health bot (dlw).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qdYE_-i4Nd7WXbAWafwqpq-Y_kl5Is9c
"""

from huggingface_hub import login

# Replace 'your_huggingface_api_token' with the token you got from Hugging Face
login("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")

!pip install transformers bitsandbytes accelerate faiss-cpu sentence-transformers



# Load your own document (assuming it's a text file where each line is a separate document)
with open('/content/dlw dataset1.csv', 'r', encoding='utf-8') as file:
    documents = file.readlines()

# Clean up the document
documents = [doc.strip() for doc in documents if doc.strip()]

import faiss
from sentence_transformers import SentenceTransformer

# Load a lightweight embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Convert documents to embeddings (as a NumPy array, not a tensor)
doc_embeddings = embedder.encode(documents, convert_to_numpy=True)

# Set up FAISS index for fast similarity search
embedding_dimension = doc_embeddings.shape[1]  # Dimension of the embeddings
index = faiss.IndexFlatL2(embedding_dimension)  # L2 distance index for FAISS

# Normalize embeddings for FAISS (FAISS works better with normalized vectors)
faiss.normalize_L2(doc_embeddings)

# Add embeddings to the FAISS index
index.add(doc_embeddings)

def retrieve_relevant_docs(query, k=2):
    query_embedding = embedder.encode([query], convert_to_numpy=True)  # Ensure NumPy array
    faiss.normalize_L2(query_embedding)  # Normalize query embedding for better FAISS performance
    distances, indices = index.search(query_embedding, k)  # Search FAISS index
    return [documents[idx] for idx in indices[0]]  # Return top-k matching documents

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model & tokenizer
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")  # Avoids padding issues
model = AutoModelForCausalLM.from_pretrained(model_name)

# Ensure model runs efficiently on CPU (or change to GPU if needed)
model = model.to("cpu")  # Change to .to("cuda").half() if running on GPU

import torch

def generate_response(query, retrieved_docs):
    # Ensure tokenizer has a padding token (important for DistilGPT-2)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Combine retrieved docs and query
    context = " ".join(retrieved_docs) if retrieved_docs else "General conversation."
    input_text = f"Context: {context}\nUser: {query}\nBot:"

    # Tokenize input properly
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,  # Prevents long inputs
        max_length=512  # Ensures input fits model
    ).to(model.device)

    # Generate response with improved settings
    outputs = model.generate(
        **inputs,
        max_length=150,
        pad_token_id=tokenizer.pad_token_id,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        top_k=50,  # More focused responses
        repetition_penalty=1.2  # Avoids repetitive text
    )

    # Decode and return only bot's response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()

import logging

# Configure logging for debugging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def chatbot(query):
    try:
        retrieved_docs = retrieve_relevant_docs(query)  # Now handles empty results internally
        logging.info(f"Retrieved Documents: {retrieved_docs}")

        # Generate a response using DistilGPT-2
        return generate_response(query, retrieved_docs)

    except Exception:
        logging.error("Error in chatbot function", exc_info=True)
        return "Sorry, I encountered an issue. Can you try again?"

import logging

def retrieve_relevant_docs(query, k=2):
    try:
        # Get the query embedding and ensure it is a NumPy array
        query_embedding = embedder.encode([query], convert_to_numpy=True)

        # Copy and normalize embedding for FAISS search
        query_embedding_norm = query_embedding.copy()
        faiss.normalize_L2(query_embedding_norm)

        # Perform FAISS search to get top-k relevant documents
        distances, indices = index.search(query_embedding_norm, k)

        # Handle empty results
        if len(indices[0]) == 0:
            return ["Sorry, I couldn’t find anything relevant."]

        # Retrieve the corresponding documents based on FAISS search results
        return [documents[idx] for idx in indices[0]]

    except Exception as e:
        logging.error("Error in document retrieval", exc_info=True)
        return ["I'm having trouble retrieving information right now."]

def generate_response(query, retrieved_docs):
    context = " ".join(retrieved_docs)
    input_text = f"Context: {context}\nUser: {query}\nBot:"

    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()

from transformers import BartForConditionalGeneration, BartTokenizer

# Load BART summarization model
summarizer_model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
summarizer_tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

# Function to summarize long text using BART
def summarize_text(text, max_length=150):
    if len(text) > 300:  # Summarize only if text is too long
        inputs = summarizer_tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = summarizer_model.generate(inputs["input_ids"], max_length=max_length, min_length=50, length_penalty=2.0, num_beams=4)
        return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return text

# Retrieve and summarize relevant documents
def retrieve_and_summarize(query, k=1):
    retrieved_docs = retrieve_relevant_docs(query, k)
    summarized_docs = [summarize_text(doc) for doc in retrieved_docs]
    return summarized_docs

def chatbot(query):
    try:
        # Retrieve and summarize relevant documents
        summarized_docs = retrieve_and_summarize(query, k=1)

        if not summarized_docs:
            summarized_docs = ["I'm here to help! What would you like to know?"]

        logging.info(f"Retrieved and Summarized Docs: {summarized_docs}")

        # Generate a response using the improved function
        response = generate_response(query, summarized_docs)
        return response

    except Exception as e:
        logging.error(f"Error in chatbot function: {str(e)}")
        return "Sorry, I encountered an issue. Can you try again?"



def generate_response(query, retrieved_docs):
    # Combine retrieved documents and the query as context
    context = " ".join(retrieved_docs)

    # Refine the prompt to be more specific and professional
    input_text = (
        f"Context: {context}\n"
        f"User: {query}\n"
        "Bot: Based on the provided context, please recommend a suitable course in a professional manner."
    )

    # Tokenize the input and generate response
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate new tokens with a lower temperature and top-k sampling for more focused responses
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,  # Limit the number of generated tokens
        temperature=0.7,     # Lower temperature for more focused output
        top_k=50             # Use top-k sampling
    )

    # Decode the generated text into a human-readable format
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()  # Extract the bot's response

def generate_response(query, retrieved_docs):
    # Combine retrieved documents and the query as context
    context = " ".join(retrieved_docs)

    # Refine the prompt for a health chatbot
    input_text = (
        f"Context: {context}\n"
        f"User: {query}\n"
        "Bot: Please provide a clear, medically relevant response based on the given context. Keep it simple and helpful."
    )

    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate response with controlled randomness
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,  # Limit response length
        temperature=0.7,      # Controls creativity (lower is more precise)
        top_k=50              # Limits unpredictable words
    )

    # Decode and return only the bot’s response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()

# Function to generate a health-related response using DistilGPT-2
def generate_response(query, retrieved_docs):
    # Combine retrieved documents and the query as context
    context = " ".join(retrieved_docs)

    # Provide instruction for a health-focused response
    input_text = (
        f"Context: {context}\n"
        f"User: {query}\n"
        "Bot: Please provide a medically relevant, easy-to-understand response based on the given information."
    )

    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate response with controlled randomness
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,  # Limit response length
        temperature=0.6,      # Lower temperature for more factual responses
        top_k=50              # Focus on top 50 token choices
    )

    # Decode and return only the bot’s response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()

def retrieve_relevant_docs(query, k=2):
    medical_docs = [
        "Flu symptoms include fever, chills, cough, and body aches.",
        "A sore throat can be treated with warm salt water gargles, honey, and hydration.",
        "Diabetes requires regular monitoring of blood sugar levels.",
        "Hypertension can be managed with a balanced diet and exercise.",
        "A mild fever is usually not serious, but if it lasts more than a few days, see a doctor."
    ]

    # Basic keyword search (case insensitive)
    relevant_docs = [doc for doc in medical_docs if any(word in doc.lower() for word in query.lower().split())]

    return relevant_docs[:k] if relevant_docs else ["Sorry, I couldn't find anything relevant."]

pip install openai faiss-cpu numpy

import re
import pandas as pd
from tabulate import tabulate
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load conversational language model (GPT-2)
mental_health_model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(mental_health_model_name)
model = AutoModelForCausalLM.from_pretrained(mental_health_model_name)

# Load zero-shot classification model for intent recognition
intent_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
intents = ["anxiety support", "stress relief", "panic attack help", "general mental health question"]

# Global variable to maintain conversation history
conversation_history = ""

# Function to classify the user's query using intent detection
def classify_query(query):
    result = intent_classifier(query, intents)
    top_intent = result['labels'][0]

    # Map the intent to a response type
    if top_intent == "anxiety support" or top_intent == "panic attack help":
        return "urgent"
    elif top_intent == "stress relief":
        return "relaxation"
    else:
        return "conversational"

# Function to generate a conversational response using GPT-2
def generate_conversational_response(query, conversation_history):
    full_input = conversation_history + "\nUser: " + query + "\nBot:"
    inputs = tokenizer(full_input, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=150, do_sample=True, top_k=50, top_p=0.95)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Function to generate a response based on classification
def generate_response(query, conversation_history):
    response_type = classify_query(query)

    if response_type == "urgent":
        return "I hear that you're feeling overwhelmed. Try the 5-4-3-2-1 grounding technique: focus on 5 things you see, 4 you can touch, 3 you hear, 2 you smell, and 1 you taste. You're not alone. I'm here to listen."
    elif response_type == "relaxation":
        return "Deep breathing can help. Try inhaling for 4 seconds, holding for 4 seconds, and exhaling for 6 seconds. Do you want a guided breathing exercise?"
    else:
        return generate_conversational_response(query, conversation_history)

# Main chatbot function
def mental_health_chatbot(query):
    global conversation_history
    response = generate_response(query, conversation_history)
    conversation_history += "\nUser: " + query + "\nBot: " + response
    return response

# Example Usage
query = "I'm feeling really anxious right now."
response = mental_health_chatbot(query)
print(response)

import re
import pandas as pd
from tabulate import tabulate
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load GPT-2 for conversational responses
mental_health_model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(mental_health_model_name)
model = AutoModelForCausalLM.from_pretrained(mental_health_model_name)

# Load zero-shot classification model for intent recognition
intent_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
intents = [
    "anxiety support", "stress relief", "panic attack help",
    "depression support", "sleep problems", "general mental health question"
]

# Global variable to maintain conversation history
conversation_history = ""

# Function to classify the user's query using intent detection
def classify_query(query):
    result = intent_classifier(query, intents)
    top_intent = result['labels'][0]

    # Map the intent to a response type
    if top_intent in ["anxiety support", "panic attack help", "depression support"]:
        return "urgent"
    elif top_intent == "stress relief":
        return "relaxation"
    elif top_intent == "sleep problems":
        return "sleep"
    else:
        return "conversational"

# Function to generate a conversational response using GPT-2
def generate_conversational_response(query, conversation_history):
    prompt_template = (
        "You are an empathetic mental health support chatbot. Your goal is to provide comforting, practical, and helpful responses.\n"
        "If the user is feeling anxious, stressed, or overwhelmed, offer evidence-based coping strategies.\n"
        "Keep responses brief, supportive, and informative.\n"
        f"Conversation history:\n{conversation_history}\nUser: {query}\nBot:"
    )

    inputs = tokenizer(prompt_template, return_tensors="pt")
    outputs = model.generate(
        inputs["input_ids"],
        max_length=150,
        do_sample=True,
        top_p=0.8,
        temperature=0.7,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.eos_token_id  # Prevents warning
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Bot:")[-1].strip()

# Function to generate a response based on classification
def generate_response(query, conversation_history):
    response_type = classify_query(query)

    if response_type == "urgent":
        return (
            "I'm really sorry you're feeling this way. You're not alone, and I'm here to help. "
            "Try the 5-4-3-2-1 grounding technique: "
            "Focus on 5 things you see, 4 you can touch, 3 you hear, 2 you smell, and 1 you taste. "
            "Would you like some guided breathing exercises to calm down?"
        )
    elif response_type == "relaxation":
        return (
            "Stress can be overwhelming, but deep breathing can help. "
            "Try the 4-7-8 technique: Inhale for 4 seconds, hold for 7 seconds, and exhale for 8 seconds. "
            "Would you like a mindfulness exercise?"
        )
    elif response_type == "sleep":
        return (
            "Having trouble sleeping? Try a bedtime routine: "
            "1) Reduce screen time 30 minutes before bed, "
            "2) Listen to calm music, "
            "3) Try progressive muscle relaxation. "
            "Would you like a guided sleep meditation?"
        )
    else:
        return generate_conversational_response(query, conversation_history)

# Main chatbot function
def mental_health_chatbot(query):
    global conversation_history
    response = generate_response(query, conversation_history)
    conversation_history += f"\nUser: {query}\nBot: {response}"
    return response

# Example Usage
query = "I'm feeling really anxious right now."
response = mental_health_chatbot(query)
print("Chatbot Response:\n", response)

# Example Usage
query = "i am under trauma exams pressure are so high that i am stressed i habe no one ton talk to additionally my parenmsty are not taking my calls as they are busy with workm help"
response = mental_health_chatbot(query)
print("Chatbot Response:\n", response)



user_input ="i am under trauma exams pressure are so high that i am stressed i habe no one ton talk to additionally my parenmsty are not taking my calls as they are busy with workm help "
response = chatbot(user_input)
print("Chatbot Response:\n", response)